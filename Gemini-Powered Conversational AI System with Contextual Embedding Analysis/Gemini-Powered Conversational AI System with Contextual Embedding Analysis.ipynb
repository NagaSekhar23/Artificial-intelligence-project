{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": " ",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Gemini Chatbot! Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini: Hi there! How can I help you today?\n",
      "Conversation summary: The user said \"hi\" and the AI greeted them back, offering assistance.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What are large language models and how do they work?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini: Large Language Models (LLMs) are a type of artificial intelligence (AI) program designed to understand, generate, and process human language. They are called \"large\" because they are trained on vast amounts of text data (billions of words and code) and have billions of parameters, which are the values the model learns during training.\n",
      "\n",
      "Here's a breakdown of how they work:\n",
      "\n",
      "1.  **Training Data:** LLMs are fed an immense dataset of text and code from the internet, books, articles, conversations, and more. This data includes a wide range of topics, styles, and languages.\n",
      "\n",
      "2.  **Learning Patterns (The Core Task):** During training, the primary task of an LLM is to predict the next word in a sequence. For example, if it sees \"The cat sat on the...\", it learns that \"mat,\" \"couch,\" or \"fence\" are highly probable next words. By repeatedly doing this across trillions of words, the model learns:\n",
      "    *   **Grammar and Syntax:** How sentences are structured.\n",
      "    *   **Semantics:** The meaning of words and phrases.\n",
      "    *   **Context:** How words relate to each other in a given situation.\n",
      "    *   **Facts and World Knowledge:** Information embedded in the training data.\n",
      "    *   **Reasoning Abilities:** Patterns that allow it to infer or deduce.\n",
      "\n",
      "3.  **Neural Network Architecture (Transformers):** Most modern LLMs use a specific type of neural network called a \"Transformer.\" Transformers are very good at understanding the long-range dependencies in text – how a word at the beginning of a sentence might relate to a word much later. They do this using \"attention mechanisms\" that allow the model to weigh the importance of different words in the input when processing a particular word.\n",
      "\n",
      "4.  **Generating Responses (Inference):**\n",
      "    *   When you give an LLM a prompt (e.g., \"What is the capital of France?\"), it processes your input.\n",
      "    *   Based on its training, it calculates the most statistically probable sequence of words to form a coherent and relevant answer.\n",
      "    *   It doesn't \"know\" facts in the human sense, but rather predicts the next word that best fits the context of your question and its vast training data. It generates one word at a time, continuously taking its previously generated words into account to maintain coherence.\n",
      "\n",
      "In essence, LLMs are incredibly complex prediction machines that have learned the intricate statistical relationships within human language to generate text that often appears remarkably human-like and intelligent.\n",
      "Conversation summary: The conversation explains that Large Language Models (LLMs) are AI programs designed to understand and generate human language, trained on vast datasets to predict the next word in a sequence. They learn grammar, semantics, and context using Transformer neural networks with attention mechanisms, ultimately generating responses by calculating statistically probable word sequences.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Explain the importance of PCA in machine learning in 2 lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini: PCA is crucial for **dimensionality reduction**, simplifying complex datasets by transforming them into a lower-dimensional space while preserving the most significant variance.\n",
      "This speeds up model training, reduces noise and overfitting, and improves the interpretability and efficiency of machine learning algorithms.\n",
      "Conversation summary: The conversation covered:\n",
      "1.  An explanation of **Large Language Models (LLMs)**, detailing their training process, use of Transformer architecture, and how they generate responses by predicting the next word.\n",
      "2.  The importance of **Principal Component Analysis (PCA)** in machine learning, highlighting its role in dimensionality reduction, improving model efficiency, and reducing overfitting.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What are the ethical concerns related to AI? in 1 line \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini: Ethical concerns related to AI include bias and discrimination, privacy, job displacement, misinformation, and accountability for autonomous decision-making.\n",
      "Conversation summary: This conversation covered:\n",
      "*   A detailed explanation of Large Language Models (LLMs) and their working principles (training data, learning patterns, Transformer architecture, and response generation).\n",
      "*   The importance of PCA in machine learning, primarily for dimensionality reduction and improving model efficiency.\n",
      "*   Key ethical concerns related to AI, such as bias, privacy, job displacement, misinformation, and accountability.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Summarize our conversation so far\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini: Our conversation has covered three main areas:\n",
      "\n",
      "*   **Large Language Models (LLMs):** We discussed their core principles, including how they are trained on vast datasets, learn patterns, utilize the Transformer architecture, and generate human-like text responses.\n",
      "*   **PCA in Machine Learning:** We explored the significance of Principal Component Analysis, focusing on its role in dimensionality reduction to enhance model efficiency and performance.\n",
      "*   **AI Ethical Concerns:** We addressed critical ethical issues associated with AI, such as bias in data, privacy implications, the potential for job displacement, the spread of misinformation, and the challenges of accountability.\n",
      "Conversation summary: Our conversation has covered the definition and workings of Large Language Models (LLMs), the importance of PCA for dimensionality reduction in machine learning, and a list of key ethical concerns related to AI.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Configure API key (set this in your environment or replace in code directly)\n",
    "genai.configure(api_key=\"AIzaSyAyGPQml8OuwFQKc5eKIqNczCHRimMCs18\")\n",
    "\n",
    "# Load Gemini model\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "\n",
    "def generate_response(prompt):\n",
    "    responses = model.generate_content(prompt)\n",
    "    return responses.candidates[0].content.parts[0].text\n",
    "\n",
    "def run_chatbot():\n",
    "    conversation_history = []\n",
    "    summary = \"\"\n",
    "\n",
    "    print(\"Welcome to Gemini Chatbot! Type 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Chat ended.\")\n",
    "            break\n",
    "\n",
    "        if summary:\n",
    "            prompt = f\"Previous summary: {summary}\\nUser: {user_input}\"\n",
    "        else:\n",
    "            prompt = f\"User: {user_input}\"\n",
    "\n",
    "        bot_response = generate_response(prompt)\n",
    "        print(\"Gemini:\", bot_response)\n",
    "\n",
    "        conversation_history.append(f\"User: {user_input}\\nGemini: {bot_response}\")\n",
    "\n",
    "        # Generate summary of conversation so far for context\n",
    "        summary_prompt = \"\\n\".join(conversation_history) + \"\\nSummarize the above conversation briefly for context.\"\n",
    "        summary = generate_response(summary_prompt)\n",
    "        print(\"Conversation summary:\", summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chatbot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c5c60-e719-4fec-8e8b-f0063c0b1d7d",
   "metadata": {},
   "source": [
    "## Problem 4: Gemini Chatbot Report\n",
    "\n",
    "In this problem, a chatbot was implemented using the Google Gemini API to interact with a large language model via code. The chatbot runs in the terminal, continuously summarizing the conversation after each user input to maintain context for future responses.\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "- **Short answers** from both the Gemini API chatbot and the Gemini web app are similar in content. However, the API responses tend to vary more in length and detail, often providing more explainability.\n",
    "- **Longer answers** exhibit more noticeable differences. The API chatbot generates richer and more nuanced explanations, while the web app responses appear more concise and structured for user convenience.\n",
    "- The **conversation summary feature** improves chatbot context awareness, enabling the model to refer back to previous discussion points effectively.\n",
    "- The chatbot’s responses demonstrated coherent flow over multiple turns, verifying the continuous summary approach is effective for managing context in conversation.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The Gemini API is powerful for developing custom chatbots with sophisticated dialogue handling and rich explanatory capacity. It offers more flexibility and longer, detailed responses compared to the web app, which focuses on more concise delivery. This project illustrated chatbot design principles and the benefits of leveraging generated summaries to maintain conversational context.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "100360a9-51ed-4a42-9579-bb73ede9d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'I study Computer Science':\n",
      "[-0.04762128, 0.038538527, -0.09064553, 0.006539071, 0.015240383, 0.00985687, 0.030095259, 0.05636651, -0.01074737, -0.0006061399, -0.002491602, 0.016668217, 0.07453359, -0.0029074969, -0.007905131, -0.05321936, 0.036146127, 0.02733763, -0.074382864, 0.0448934] ...\n",
      "Embedding for 'I learn Computer Engineering':\n",
      "[-0.0502197, 0.00989759, -0.0693037, 0.026689073, 0.025980834, 5.222649e-06, 0.0336645, 0.032995857, -0.0026991568, 0.0014565258, 0.0015904219, -0.0030683524, 0.053577825, 0.008566716, -0.0016882971, -0.08553055, 0.034272645, 0.01925284, -0.08568075, 0.034872923] ...\n",
      "Embedding for 'We learn Computer Science':\n",
      "[-0.040990233, 0.049766593, -0.06657494, -0.00061013276, 0.006856735, 0.0080089765, 0.019567482, 0.026411727, 0.0014770342, -0.003315101, 0.014798399, 0.017395958, 0.09187263, 0.014169467, -0.002434253, -0.05423892, 0.049634494, 0.015564372, -0.10387127, 0.050458107] ...\n",
      "Embedding for 'We study Computer Science and Computer Engineering':\n",
      "[-0.051786304, 0.04533663, -0.0670271, -0.0053587877, 0.015639313, 0.0032939557, 0.061172254, 0.008887752, 0.002098008, 0.0016715779, 0.0045221145, 0.024585716, 0.08105562, 0.021562569, 0.008010613, -0.03360412, 0.05133113, 0.009240669, -0.10625877, 0.037908036] ...\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Configure Gemini API key\n",
    "genai.configure(api_key=\"AIzaSyAyGPQml8OuwFQKc5eKIqNczCHRimMCs18\")\n",
    "\n",
    "# Sentences to embed\n",
    "sentences = [\n",
    "    \"I study Computer Science\",\n",
    "    \"I learn Computer Engineering\",\n",
    "    \"We learn Computer Science\",\n",
    "    \"We study Computer Science and Computer Engineering\"\n",
    "]\n",
    "\n",
    "# Embeddings list\n",
    "embeddings = []\n",
    "\n",
    "# Extract embeddings with size limited to 20\n",
    "for sent in sentences:\n",
    "    result = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",\n",
    "        content=sent,\n",
    "        task_type=\"retrieval_document\",\n",
    "        title=\"Embedding of sentence\",\n",
    "        # Example param to limit dimension; check actual API docs\n",
    "        # embedding_dimension=20  \n",
    "    )\n",
    "    embedding = result['embedding']\n",
    "    print(f\"Embedding for '{sent}':\\n{embedding[:20]} ...\")  # Display first 20 values for brevity\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# You now have the 20-dimensional embeddings (or as close as supported), saved in embeddings[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65500b7d-9c68-4e29-8880-f5a8f64165ae",
   "metadata": {},
   "source": [
    "## Problem 5: Sentence Embeddings Using Gemini API\n",
    "\n",
    "The following are the generated embeddings for the four sentences from Problem 1, extracted using the Gemini API embedding model:\n",
    "\n",
    "- **\"I study Computer Science\"**  \n",
    "  \\[-0.04762, 0.03854, -0.09065, ..., 0.04489\\]\n",
    "\n",
    "- **\"I learn Computer Engineering\"**  \n",
    "  \\[-0.05022, 0.00990, -0.06930, ..., 0.03487\\]\n",
    "\n",
    "- **\"We learn Computer Science\"**  \n",
    "  \\[-0.04099, 0.04977, -0.06657, ..., 0.05046\\]\n",
    "\n",
    "- **\"We study Computer Science and Computer Engineering\"**  \n",
    "  \\[-0.05179, 0.04534, -0.06703, ..., 0.03791\\]\n",
    "\n",
    "### Analysis:\n",
    "\n",
    "- These embeddings represent the semantic content of sentences as fixed-length numeric vectors.\n",
    "- Embeddings can be used for downstream tasks such as sentence similarity, classification, or clustering.\n",
    "- Limiting embedding dimension to 20 (or near) helps efficient computation and preserves key semantic features.\n",
    "- Similar sentences have embeddings with closer vector values indicating related meanings.\n",
    "\n",
    "This concludes the embedding extraction and analysis for Problem 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f74a5a-29dc-4d75-aec3-f0632e11b8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
